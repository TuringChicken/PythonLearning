# natural language
import jieba
# cut方法有两个参数
# 第一个参数是我们想分词的字符串
# 第二个参数cut_all是用来控制是否采用全模式
# 全模式
word_list = jieba.cut("今天天气真好。亲爱的，我们去远足吧！", cut_all=True)
print("全模式:", "|".join(word_list))
# 精确模式，默认就是精确模式
word_list = jieba.cut("今天天气真好。亲爱的，我们去远足吧！", cut_all=False)
print("精确模式:", "|".join(word_list))
# 搜索引擎模式
word_list = jieba.cut_for_search("今天天气真好。亲爱的，我们去远足吧！")
print("搜索引擎模式:", "|".join(word_list))

# 统计《解忧杂货店》小说词频前20的词
f = open("打字练习.txt", "rb")
txt = f.read()
words = jieba.lcut(txt)
counts = {}
for word in words:
    if len(word) == 1:
        continue
    else:
        counts[word] = counts.get(word, 0)+1
items = list(counts.items())
items.sort(key=lambda items: items[1], reverse=True)
for i in range(20):
    word, count = items[i]
    print("{0:<10}{1:>5}".format(word, count))
